---
jupyter:
  jupytext:
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.8
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Lesson 3


# Random variables to random vectors to random functions

In this lesson we'll progress through random variables to random vectors to random functions.  We'll find at the end that we've fit our first GP!

```python
import arviz as az
import aesara.tensor as at
import pandas as pd
import pymc as pm
import pymc.sampling_jax
import matplotlib.pyplot as plt
import numpy as np
from scipy import stats
```

# Section 10: Random Variables vs Random Vectors
Getting multidimensional


## Random Variable Refresher


### Equation
$\theta \text{~} N(10, 1)$


### Plot

```python
theta = pm.draw(pm.Normal.dist(mu=10, sigma=1), 50_000)

az.plot_dist(theta);
```

By now the idea of a random variable should be quite familiar. A random variable is one who's exact outpput we don't know exactly, but is defined by some sort of distribution. Here is a random variable X defined in both math notation, and also defined computationally. The cool part of the computational one is we don't just have to look at it, we can do things like draw 10k random samples and plot them!


## Now let's generalize to random vectors

```python
# Make three random variables, put them into a vector
theta1 = pm.Normal.dist(mu=4, sigma=1)
theta2 = pm.Normal.dist(mu=2, sigma=2)
theta3 = pm.Normal.dist(mu=6, sigma=0.5)

# Stick the three random variables together into a random vector
random_vector = [theta1, theta2, theta3]

# 3 random values from one draw in a random vector
random_vector
```

In this first example, the distinction between random vector and random variable is just semantic.  A random vector at it's simplest is just random variables stuck together. 

What you should pay attention too though is the shape! Each time we take draw a sample from this random vector we **always** get three random values



## Plotting Random vectors (Generated by SciPy)

```python
colors = ["C0", "C1", "C3"]
samples = stats.norm([1,2,3], [1, .75, .5]).rvs((1000,3))
```

```python
for i in range(3):
    az.plot_dist(samples[:, i], color=colors[i], label="theta" + str(i+1))

plt.title("Histogram of each element in our random vector");
plt.xlim([-3, 5]);
plt.ylim([0.0, 0.9]);
```

This is cool, now we have three normal distributions and we can plot each of them, but you be asking, why do I care? Isn't this just the same thing as if I created 3 normal variables and plotted each one, and the answer is yes

Random vectors are so useful that there's actually APIs to draw random vectors in numpy, scipy, and PyMC all shown here. Mathematically they all represent the same thing. Computationally they're provided in all libraries for convenience, but in the PyMC case also to facililate additional computations such as calculating quantities like gradients. If you're interseted in that go take the MCMC class.

Suffice to say, random vectors are quite common so its best you get used to them, especially if you're in the GP this GP class.


## Shape of random variables


* (1)
* (109,2)
* (50, 4, 2)


Once we start moving into higher dimensions one thing to pay attention to is shapes. Shapes tell you how many random values you have and how they're structured. For instance a random variable of shape (2,3) is different than one (3,2). Now right now


## 1 Factory Line making 200 donuts 


Insert picture of donut machine

```python
donut_radius = stats.norm(2, .1).rvs()
donut_radius
```

Imagine if we have a donut machine where we pull a handle and each time we get a jelly filled donut. Each time we get a donut that is like the last one but subtly different. Maybe the radius is a little bit bigger or smaller, or maybe there's more jelly or less 

Either way we now have a delicious random number generator and each time we pull the crank we get a random variable,


 ## 1 Donut maker making batches of 2 donuts in parallel, 100 times in parallel


Insert donut machine but with two donuts in parallel

```python
donut_radius = stats.norm([2,2], .1).rvs()
donut_radius
```

Now imagine our donut business is successful, and we replace our donut machine with one that produces two donuts with each pull of the crank. if we pull the crank 100 times, we still get 200 donuts, but now the randomness is different. We have a random vector generator because each time we pull the crank we get 


## The meaning of dimensions



**Insert some picture here**


What this also means is that semantically the dimensions can take on some meaning. In this example the shapes mirror our donut production. 1 "draw" gets us a (1,) sample from machine 1, where are one draw from machine 2 gets us two donuts.

But right now you may ask, whats the difference in the code between 200 random draws from donut machine our single donut machine, to 100 random draws from our second donut, resulting in 200 donuts. And the answer is nothing. You'll see it in an exercise.

**Insert an exercise where folks draw random samples from uncorrelated samples and plot the dimensions, and then one where folks draw correlated samples from an MVN and plot the marginals**



##  Independence assumptions


**Insert indepdene notation here**


So you're probably thinking If everything is independent random vectors whats the point. To become a random vector, all we did was stick them together into a list and start thinking about them as something that works together as one "unit". 

Now there is a key difference between our currently simulated donut machine and real life. In our second donut machine each donut is correlated with its pair. But we havent represented that here.

Usually , there's not much of a point of talking about random vectors when their random variables have no relationship. And in many cases, like our donut example, they do. But there are many situations where things are related, so lets get to that.


## Random vectors in PyMC, SciPY, and Numpy (Hide)

```python
# Draw samples from the random vector
samples = pm.draw(at.as_tensor_variable(random_vector), draws=10_000)
print("Shape of samples:", samples.shape)
```

## Section Recap
* Random variables are mathematical concepts typically denoted $x$
  * A single random draw is one _realized_ number 
* Random variables can be grouped together in random vectors, typically denoted in uppercase as $X$
  * A single random draw generates a vector of realized numbers
  * In our case we got 3
* Random variables can be represented as objects
  * In PyData stack typically scipy and numpy can bve used to instantiate random variables
  * We use PyTensor as it provides extra functionality for Bayesian inference



# Sec 20: Multivariate normals
Getting multidimensional with covariances



## Your first Multivariate Normal

```python
mu = 0.0
sigma = 1.0

x1 = pm.draw(pm.Normal.dist(mu=mu, sigma=sigma), 1000)
x2 = pm.draw(pm.Normal.dist(mu=mu, sigma=sigma), 1000)

axs = az.plot_pair(
    {"x1": x1, "x2": x2},
    kind=["scatter", "kde"],
    marginals=True,
    figsize=(8, 9),
); axs = axs.flatten()

axs[2].set_xlim([-5, 5])
axs[2].set_ylim([-5, 5]);
```

The most important type of random vector for this course is the multivariate normal.  **A multivariate normal distribution is a set of normal random, like the above example, but, they have correlations between each other -- they are not independent**.  In this section, we are going to intuitively describe multivariate normals.  Like kernels, MvNs are a key building block in Gaussian processes.  They're the "Gaussian" part, after all.  

It's important to start by saying that multivariate normals are a generalization of univariate normals.  Meaning, it's equivalent to describe a set of univariate (like the ones above) distributions as a special type of the multivariate case.

What does this look like?  Let's start with the simplest case.  A two dimensional multivariate normal.  We'll start here because it's possible to visualize it in two dimensions.  Lets plot samples for two normal random variables that are independent on a scatterplot.  


## The Math

<!-- #region -->


$$
p(x \mid \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left[ \frac{(x - \mu)^2}{2\sigma^2} \right]
$$

<!-- #endregion -->

Notice that I took care to make the figure a square, and to set the axis limits.  Feel free to rerun this code a few times, and draw more samples.  You'll see that the scatter plot always makes a perfect circle -- if you were to sample infinitely of course.  On the top and on the right there are two other plots verifying that this is in fact two, one dimensional normals.  This is the equation for the probability density function for a univariate normal (you've probably seen this before):

You may not have noticed that factor of $2\pi$ in the denominator.  $\pi$ is of course, all about circles. There are interesting mathematical connections between normals and circles, but we won't get into that here.  You may find that some people describe multivariate normals as "spherical", or as having some amount of "sphericity".  To show why they say this, let's add a dimension and turn our 2d scatterplot into a 3d scatterplot.


## Spheres

```python
x3 = pm.draw(pm.Normal.dist(mu=mu, sigma=sigma), 1000)

fig = plt.figure()
ax = fig.add_subplot(projection='3d')

ax.scatter(x1, x2, x3, alpha=0.3);

ax.set_xlim([-5, 5])
ax.set_ylim([-5, 5]);
ax.set_zlim([-5, 5]);
```

Instead of a circle, now it's a sphere!  It's really very difficult to imagine, but try and picture in your mind what this would look like if you added, one more dimension.  You go from a circle in 2d, to a sphere in 3d, to a "sphere" in 4d (you could call it a hypersphere if you want to sound, extra nerdy).  We live in a 3 dimensional world (4 if you count time, which unlike the other three dimensions, seems to only go one way), so I don't want to say it's impossible to picture this round blob in 4 dimensions, because maybe someone out there can, but I can't.  I do think it's important to try though, and to sort of see if you can get a sense of it.


## Why this is important


**Insert picture of exponential chart reaching infinity**


It's an important concept to at least try and wrap your brain around, because spoiler alert: Gaussian processes are actually *infinite* dimensional.  It is important to have some sense of what this means when working with them and developing models with them.  Also, the result of any Bayesian model is actually a high dimensional posterior distribution.  Many hurdles that you face working with real data, flow down from the fact that high dimensional things are difficult to visualize and tricky to deal with.  Even so, having some intuition here means that you can tackle problems and that may have seemed impossible before! 

Thanks for coming to my TED talk.  Since we have work to do and lives to live, let's come back to the mortal plane.  Circles and spheres are cool and all, but let's make make a two dimensional multivariate normal, where there is some dependence between `x1` and `x2`. 


## The important parameters of MVNs


* Mean
* Sigma
* Covariance


Now mean and sigma should look familiar, but note we now don't just have one point value mean, but rather a mean for each dimension, and the same for sigma. More dimensions means more parameters values we need to fully define our distribution

You've understand the mean and variance or standard deviation parameters, but we'll need one more parameter that is new, the covariance.  **The covariance parameter describes how strongly two random variables change together -- how much they co-vary**.  


## Formally Introducing Covariance

```python
mu = [0.0, 0.0]
sigma1 = 1.0
sigma2 = 1.0
covariance = 0.7
cov = np.array([
    [sigma1, covariance],
    [covariance, sigma2]
])
samples = pm.draw(pm.MvNormal.dist(mu=mu, cov=cov), 1000)

x1 = samples[:, 0]
x2 = samples[:, 1]

fig, ax = plt.subplots(1, 1, figsize=(6, 6))
ax.scatter(x1, x2, alpha=0.3)
ax.set_xlim([-5, 5])
ax.set_ylim([-5, 5]);
ax.set_xlabel("x1");
ax.set_ylabel("x2");
```

Instead of a perfect circle, we've now got a diagonal band.  The main new thing you might see here is `cov`.  This is the covariance matrix.  The covariance matrix is a square array of numbers, so there are always an equal number of rows and columns.  There is one row and one column per dimension.  That's why `cov` in this example is two by two.  Each row and each column correspond to one of the dimensions.  

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0pky"></th>
    <th class="tg-c3ow"><span style="font-weight:bold">Dimension 1</span></th>
    <th class="tg-c3ow"><span style="font-weight:bold">Dimension 2</span></th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky"><span style="font-weight:bold">Dimension 1</span></td>
    <td class="tg-c3ow">$\sigma^2_{11}$</td>
    <td class="tg-c3ow">$\sigma^2_{12}$</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:bold">Dimension 2</span></td>
    <td class="tg-c3ow">$\sigma^2_{12}$</td>
    <td class="tg-c3ow">$\sigma^2_{22}$</td>
  </tr>
</tbody>
</table>

If you return to the scatterplot above, you'll see we used the positive value of 0.7.  The fact that this value is positive means, when x1 is large, its more likely that x2 is also large.  Lets cover the left two thirds of the plot:

** Have Fadi animate in this table**


## Why we say X2 is usually larger than X1

```python
from matplotlib.patches import Rectangle

mu = [0.0, 0.0]
sigma1 = 1.0
sigma2 = 1.0
covariance = 0.7
cov = np.array([
    [sigma1, covariance],
    [covariance, sigma2]
])
samples = pm.draw(pm.MvNormal.dist(mu=mu, cov=cov), 1000)

x1 = samples[:, 0]
x2 = samples[:, 1]

fig, ax = plt.subplots(1, 1, figsize=(6, 6))
ax.scatter(x1, x2, alpha=0.3)
ax.set_xlim([-5, 5])
ax.set_ylim([-5, 5])
ax.set_xlabel("x1");
ax.set_ylabel("x2");

ax.add_patch(Rectangle((-5, -5), 6, 10, alpha=0.8, color="k"));
```

You can see it now, when `x2` is "large", meaning here, above 1, then the points usually are larger values of `x2`.  I'm using the word "usually" on purpose.  Some amount of covariance doesn't guarantee this to be true, it just makes it more likely.  However, the larger values of covariance, the more likely this is true.  To see that this is true, let's take a look at this when the covariance is one, equal to the variance:


## Increasing the degree of "relatedness"

```python
mu = [0.0, 0.0]
sigma1 = 1.0
sigma2 = 1.0
covariance = 1.0 # change this to one.
cov_matrix = np.array([
    [sigma1, covariance],
    [covariance, sigma2]
])
samples = pm.draw(pm.MvNormal.dist(mu=mu, cov=cov_matrix), 1000)

x1 = samples[:, 0]
x2 = samples[:, 1]

fig, ax = plt.subplots(1, 1, figsize=(6, 6))
ax.scatter(x1, x2, alpha=0.3)
ax.set_xlim([-5, 5])
ax.set_ylim([-5, 5])
ax.set_xlabel("x1");
ax.set_ylabel("x2");

ax.add_patch(Rectangle((-5, -5), 6, 10, alpha=0.8, color="k"));
```

When the covariance is equal to the variance, you can see that the word "usually" turns into the word "always".  $x1 = x2$, or, if I tell you what the value of $x1$ is, you'll be able to tell me the value of $x2$. 


## Lets do it together

```python
# Live Code
mu = [0.0, 0.0]
sigma1 = 1.0
sigma2 = 1.0
covariance = 1.0 # change this to one.
cov_matrix = np.array([
    [sigma1, covariance],
    [covariance, sigma2]
])
samples = pm.draw(pm.MvNormal.dist(mu=mu, cov=cov_matrix), 1000)

x1 = samples[:, 0]
x2 = samples[:, 1]

fig, ax = plt.subplots(1, 1, figsize=(6, 6))
ax.scatter(x1, x2, alpha=0.3)
ax.set_xlim([-5, 5])
ax.set_ylim([-5, 5])
ax.set_xlabel("x1");
ax.set_ylabel("x2");

ax.add_patch(Rectangle((-5, -5), 6, 10, alpha=0.8, color="k"));
```

The best way to really understand this is to experiment.  Try choosing different values for the mean `mu`, the dimension 1 variance, the dimension 2 variance and the covariance, and then draw and plot the samples.  Here are a few important characteristics that you'll see:

- Increasing the variances stretch the distribution, but only in that particular dimension
- The variances on the diagonal must be positive
- The covariance can be negative or positive
- When you pick covariance values that are larger than the variances, you'll get a long stack trace and some message about the matrix not being positive semi-definite.  This is a mathematical condition that just means you're out of bounds with your choice of covariance.  When you get this error, decrease the value of the covariance that you just increased.


## Covariance and Correlation are related

$$
\rho_{ij} = \operatorname{Cor}(X_i,X_j) = \frac{\operatorname{Cov}(X_i,X_j)}{\sqrt{\operatorname{Var}(X_i)\operatorname{Var}(X_j)}} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\,\sigma_{jj}}}\ .
$$


* Correlation is normalized version of covariance
* -1 means exact negative
* 1 means exact positive
* 0 means no relation
* While thats good to know, doesn't really matter for GPs

So lets get out of theory and talk about where this matters in practice1


## Examples of Covariace

* Twins and height
* Hour to hour sales in a restaurant
* Any linear regression
* Donut Machines


<!-- #region -->

Let's say we run a study to see if twins have similar heights. Would we expect their heights to be correlated, moreso than two random siblings? We can use multivariate normals to understand this.




Also, think about sales at a restaurant.  If there are a lot of customers during the lunch rush, from 12 noon to 1pm.  Are there likely to be a lot of customers in the next hour?  From 1 to 2pm?  The 12 to 1pm customer count is *at least* likely to be more similar to the 1 to 2pm count, than it is to the 3 to 4pm count.

What about our donut machine? We can use covariance to express how correlated each one of our donuts are with each other!

<!-- #endregion -->

## Section Recap
* Multivariate normals are an important case of randomness
* MVNs generate random vectors of size N
* MVNs are defined by $\mu$ and $\sigma$, but also covariance matrix
* Covariance, and correlation, specify degree of relation between pairwise variables or dimensions
* In real life many things are related
  * Sibling heights due genetics
  * The number of customers in two adjacent time slots in restaurants
  * Number of Intutive Bayes courses you take and your knowledge practical Bayesian usage



Hopefully the last one is positively correlated!


#  A different way to visualize multivariate normals
Seeing in N Dimensions




## Higher Dimensions are weird


**Insert a picture**


We can't really plot multivariate normal distributions that have more than three dimensions.  Even then, it's really not easy to see what's going on, it just looks like a formless blob of dots.  Let's try a new plot.  Instead of each dimension getting it's own axis, let's plot each dimension down the axis, as if they were sequential points in time.  Here's what I mean, in three dimensions:  


## "Unrolling" a Multivariate Normal

```python
def mvn_unroller(covariance12, covariance13, covariance23):
    sigma1 = 1.0
    sigma2 = 1.0
    sigma3 = 3.0
    mu = [-1, 2, 2]


    cov_matrix = np.array([
        [sigma1, covariance12, covariance13],
        [covariance12, sigma2, covariance23],
        [covariance13, covariance23, sigma3],
    ])
    samples = stats.multivariate_normal(mean=mu, cov=cov_matrix).rvs(100)

    dimension = np.arange(len(mu))

    fig, ax = plt.subplots(1, 1, figsize=(10, 6));
    ax.plot(dimension, samples.T, color="slateblue", lw=0.5, alpha=0.25, marker="o");
    ax.set_xticks(dimension)
    ax.set_xticklabels(["Dimension " + str(i) for i in dimension + 1]);
    
mvn_unroller(covariance12=.8, covariance13=0.1, covariance23=-0.9)
```

##### What does covariance look like in this context?

- When you go from dimension 1 to dimension 2, consider the value of `covariance12`.  It's positive, and the lines basically never cross each other. As the value in dimension one increases, so does the value in dimension 2.
- Look at dimension 2 to 3, and consider the value of `covariance23`.  It's negative, and basically every line crosses.  If it's relatively high in dimension 2, it goes down and is low in dimension 3, and visa versa. 
- What about `covariance13`?  It's here, but it's invisible in this plot.  It's doing something similar to `covariance12` and `covariance13`.  But, that's the thing about projecting a higher dimensional thing down into a lower dimensional space.  Things in higher dimensions do have real effects, but these effects are invisible from lower dimensional viewpoints.  The effects they produce are real, but the mechanism isn't apparent. 

Now the best way to intuitively understand, and that this is to change numbers and see what happens. That's why we created this function, so we can do this again, with different covariances while changing nothing else



## Add another plot

```python
mvn_unroller(covariance12=.99, covariance23=.001, covariance13=.1)
```

Here's another example, and this ones a real intuition builder so lets really talk through it.

If we look at each dimension individually we have the familiar normal distribution, thats why you see the differing means and variations. But between dimensions is where things get interesting for us because that's controlled by covariance. Notice how between dimensions 1 and 2 thee lines are parallel, while between 2 and 3 they're more varied! Thats covariance in action

Again, I encourage you to play with the numbers here and see what happens.  Nothing is stopping us from making this type of plots in even higher dimensions, even hundreds or thousands of dimensions is possible.  We'll make plots of Gaussian process that look like this many many times in this course!


## 


## Multivariate normal wrap-up

**Insert picture of MVNs being wrapped in tape or soomething**


The amount of knowledge out there on normal and multivariate normals is pretty vast.  A quick skim of the wikipedia page will tell you as much.  For now though, we'll redirect ourselves back towards building a Gaussian process. 

In GPs Multivariate normals are important because of two other key concepts, **marginalization** and **conditioning**.   We will return to multivariate normals in a later lesson after we fit our first GPs.  


## Section Recap
* Multivariate Normals can, and often do, extend past 3 dimensions
* To visualize higher dimensions we can plot them in what's called a parallel plot
  * Each dimension is on the x axis
  * The y is the value of the random draws
  * A continuous line connects the vector of points together
  * These lines are not strictly parallel, making the plot name a bit unfortunate
* Adjusting covariance changed what we saw in these plots


# Remember kernels?
We did tell you it was one of the most important concepts


## The similarity between covariance and kernel 





In that last section we saw how covariance defined the similarity between two dimensions in random vectors drawn from multivariate normals.

In the last _lesson_ we talked about how kernels functions calculated the similarity between two points.

*Isn't this kind of the same thing as the covariances bwetween elements in a multivariate normal?*  


## Generating Covariance 

```python
def kernel(x, x_prime, lengthscale):
    return np.exp( -0.5 * np.square(x - x_prime) / np.square(lengthscale))

num_dims = 3
x = np.arange(num_dims)

def covariancec()
# Calculate kernel/covariance matrix values for every possible pair of x values in the data
K = np.zeros((num_dims, num_dims))
for i in range(num_dims):
    for j in range(num_dims):
        K[i, j] = kernel(x[i], x[j], lengthscale=2)
        
K
```

What if we used a kernel to generate the covariance matrix for a multivariate normal?  Remember the main takeaway from that lesson, kernel functions give the similarity of the points in $y$, depending on the points $x$ and $x'$. 

Let's try it out.  Let's take our same exponentiated quadratic kernel from Art class, use it to calculate a 10 by 10 covariance matrix, 


## Drawing samples

```python
# Lets use a zero mean this time
mu = np.zeros(10)
    
# Draw samples from this random vector
random_vector = pm.MvNormal.dist(mu=mu, cov=K)
samples = pm.draw(random_vector, draws=10)
samples[:3]
```

And use it to draw samples from a multivariate normal. This is another example of a random vector where there are relationships between the different X's.  And of course, these relationships are determined by the kernel matrix.  




```python
x = np.arange(len(mu))
fig, ax = plt.subplots(1,1,figsize=(10, 6))

# plot 100 of the samples
ix = np.random.randint(0, samples.shape[0], 100)
ax.plot(x, samples[ix, :].T, color="slateblue", lw=0.5, alpha=0.5, marker=".")

ax.set_xticks(x);
ax.set_xticklabels(["Dim " + str(i + 1) for i in range(len(x))]);
```

There's a very important quality to notice here, **smoothness**.  Just like in art class, the changes from point to point as we go from left to right are smooth.  Except now, we have multivariate normals in our toolbelt.  The combination of a kernel and a multivariate normal are the two primary ingredients in a Gaussian process.


## Insert another plot


**TODO**: 
* Put a non kernel covariance and covariance MVN draw side by side
* Highlight a random draw to show how "less" random the path looks when a kernel defines covariance

* Exercise idea: Give people a kernel, ask them to take random draws, and tell us "what the kernel is mapping to" For instance a periodic kernel


## Section Recap
* Kernels can be used to generate covariance matrices
  * Just like in art class, the kernel defines how "close" two dimensions should be
* Random vectors can be generated from covariance matrices and MVNs
  * This was quite evident once we compared to the parallel plots from the last section
